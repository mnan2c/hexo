title: SpringBoot集成Kafka

tags:
  - 类库&中间件

categories:
  - 类库&中间件

---
## 1. 引入Maven依赖
```xml
<dependency>
	<groupId>org.springframework.boot</groupId>
	<artifactId>spring-boot-starter</artifactId>
</dependency>
<dependency>
	<groupId>org.springframework.kafka</groupId>
	<artifactId>spring-kafka</artifactId>
</dependency>

<dependency>
	<groupId>org.projectlombok</groupId>
	<artifactId>lombok</artifactId>
	<optional>true</optional>
</dependency>
<dependency>
    <groupId>com.google.code.gson</groupId>
    <artifactId>gson</artifactId>
    <version>2.8.2</version>
</dependency>
```
## 2. 配置文件application.yml
```yaml
spring:
  kafka:
    #  指定kafka代理地址，可以多个
    bootstrap-servers: 127.0.0.1:9092,127.0.0.2:9092,127.0.0.3:9092

    producer:
      retries: 0
      # 每次批量发送消息的数量
      batch-size: 16384
      buffer-memory: 33554432
      key-serializer:
      # 指定消息key和消息体的编解码方式
      org.apache.kafka.common.serialization.StringSerializer
      value-serializer: org.apache.kafka.common.serialization.StringSerializer
      properties:
        linger.ms: 1

    consumer:
      group-id: test-consumer-group
      enable-auto-commit: true
      auto-commit-interval: 100ms
      key-deserializer: org.apache.kafka.common.serialization.StringDeserializer
      value-deserializer: org.apache.kafka.common.serialization.StringDeserializer
      properties:
        session.timeout.ms: 15000
```
## 2. 消息实体类
```java
import java.util.Date;

import lombok.Data;

@Data
public class Message {
  private Long id; // id
  private String msg; // 消息
  private Date sendTime; // 时间戳
}
```
## 3. 消息Producer
```java
import java.util.Date;
import java.util.UUID;

import org.springframework.beans.factory.annotation.Autowired;
import org.springframework.kafka.core.KafkaTemplate;
import org.springframework.stereotype.Component;

import com.google.gson.Gson;
import com.google.gson.GsonBuilder;

import lombok.extern.slf4j.Slf4j;

@Slf4j
@Component
public class Producer {
  @Autowired
  private KafkaTemplate<String, String> kafkaTemplate;
  private Gson gson = new GsonBuilder().create();

  // 发送消息方法
  public void send() {
    Message message = new Message();
    message.setId(System.currentTimeMillis());
    message.setMsg(UUID.randomUUID().toString());
    message.setSendTime(new Date());
    log.info("+++++++++++++++++++++  message = {}", gson.toJson(message));
    kafkaTemplate.send("zhisheng", gson.toJson(message));
  }
}

```
## 4. 消息Consumer
```java
import java.util.Optional;

import org.apache.kafka.clients.consumer.ConsumerRecord;
import org.springframework.kafka.annotation.KafkaListener;
import org.springframework.stereotype.Component;

import lombok.extern.slf4j.Slf4j;

@Slf4j
@Component
public class Consumer {

  @KafkaListener(topics = {"zhisheng"})
  public void listen(ConsumerRecord<?, ?> record) {
    Optional<?> kafkaMessage = Optional.ofNullable(record.value());
    if (kafkaMessage.isPresent()) {
      Object message = kafkaMessage.get();
      log.info("----------------- record =" + record);
      log.info("------------------ message =" + message);
    }
  }
}
```
## 5. 启动类
```java
import org.springframework.boot.SpringApplication;
import org.springframework.boot.autoconfigure.SpringBootApplication;
import org.springframework.context.ConfigurableApplicationContext;

import com.example.demo.kafka.Producer;

@SpringBootApplication
public class DemoApplication {

  public static void main(String[] args) {
    ConfigurableApplicationContext context = SpringApplication.run(DemoApplication.class, args);
    Producer sender = context.getBean(Producer.class);
    for (int i = 0; i < 3; i++) {
      // 调用消息发送类中的消息发送方法
      sender.send();
      try {
        Thread.sleep(3000);
      } catch (InterruptedException e) {
        e.printStackTrace();
      }
    }
  }

}
```
## 7. 启动日志输出
```log
2019-03-29 08:40:11.094  INFO 1192 --- [           main] o.a.k.clients.consumer.ConsumerConfig    : ConsumerConfig values:
	auto.commit.interval.ms = 100
	auto.offset.reset = latest
	bootstrap.servers = [127.0.0.1:9092, 127.0.0.2:9092, 127.0.0.3:9092]
	check.crcs = true
	client.id =
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 15000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2019-03-29 08:40:11.134  INFO 1192 --- [           main] o.a.kafka.common.utils.AppInfoParser     : Kafka version : 2.0.1
2019-03-29 08:40:11.134  INFO 1192 --- [           main] o.a.kafka.common.utils.AppInfoParser     : Kafka commitId : fa14705e51bd2ce5
2019-03-29 08:40:11.214  INFO 1192 --- [           main] org.apache.kafka.clients.Metadata        : Cluster ID: JZupJV5GSruEvP9tOLpUFA
2019-03-29 08:40:11.394  INFO 1192 --- [           main] o.a.k.clients.consumer.ConsumerConfig    : ConsumerConfig values:
	auto.commit.interval.ms = 100
	auto.offset.reset = latest
	bootstrap.servers = [127.0.0.1:9092, 127.0.0.2:9092, 127.0.0.3:9092]
	check.crcs = true
	client.id =
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = test-consumer-group
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	session.timeout.ms = 15000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

2019-03-29 08:40:11.404  INFO 1192 --- [           main] o.a.kafka.common.utils.AppInfoParser     : Kafka version : 2.0.1
2019-03-29 08:40:11.404  INFO 1192 --- [           main] o.a.kafka.common.utils.AppInfoParser     : Kafka commitId : fa14705e51bd2ce5
2019-03-29 08:40:11.404  INFO 1192 --- [           main] o.s.s.c.ThreadPoolTaskScheduler          : Initializing ExecutorService
2019-03-29 08:40:11.414  INFO 1192 --- [           main] com.example.demo.DemoApplication         : Started DemoApplication in 1.192 seconds (JVM running for 1.757)
2019-03-29 08:40:11.414  INFO 1192 --- [ntainer#0-0-C-1] org.apache.kafka.clients.Metadata        : Cluster ID: JZupJV5GSruEvP9tOLpUFA
2019-03-29 08:40:11.424  INFO 1192 --- [ntainer#0-0-C-1] o.a.k.c.c.internals.AbstractCoordinator  : [Consumer clientId=consumer-2, groupId=test-consumer-group] Discovered group coordinator CC4KJZT4L672A9L:9092 (id: 2147483647 rack: null)
2019-03-29 08:40:11.424  INFO 1192 --- [           main] com.example.demo.kafka.Producer          : +++++++++++++++++++++  message = {"id":1553820011414,"msg":"f9d1fb4e-cbaa-43de-adf3-ccc90eb98433","sendTime":"Mar 29, 2019 8:40:11 AM"}
2019-03-29 08:40:11.424  INFO 1192 --- [ntainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-2, groupId=test-consumer-group] Revoking previously assigned partitions []
2019-03-29 08:40:11.424  INFO 1192 --- [ntainer#0-0-C-1] o.s.k.l.KafkaMessageListenerContainer    : partitions revoked: []
2019-03-29 08:40:11.424  INFO 1192 --- [ntainer#0-0-C-1] o.a.k.c.c.internals.AbstractCoordinator  : [Consumer clientId=consumer-2, groupId=test-consumer-group] (Re-)joining group
2019-03-29 08:40:11.424  INFO 1192 --- [           main] o.a.k.clients.producer.ProducerConfig    : ProducerConfig values:
	acks = 1
	batch.size = 16384
	bootstrap.servers = [127.0.0.1:9092, 127.0.0.2:9092, 127.0.0.3:9092]
	buffer.memory = 33554432
	client.id =
	compression.type = none
	connections.max.idle.ms = 540000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 1
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.1, TLSv1]
	ssl.endpoint.identification.algorithm = https
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLS
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

2019-03-29 08:40:11.434  INFO 1192 --- [           main] o.a.kafka.common.utils.AppInfoParser     : Kafka version : 2.0.1
2019-03-29 08:40:11.434  INFO 1192 --- [           main] o.a.kafka.common.utils.AppInfoParser     : Kafka commitId : fa14705e51bd2ce5
2019-03-29 08:40:11.444  INFO 1192 --- [ad | producer-1] org.apache.kafka.clients.Metadata        : Cluster ID: JZupJV5GSruEvP9tOLpUFA
2019-03-29 08:40:11.524  INFO 1192 --- [ntainer#0-0-C-1] o.a.k.c.c.internals.AbstractCoordinator  : [Consumer clientId=consumer-2, groupId=test-consumer-group] Successfully joined group with generation 1
2019-03-29 08:40:11.524  INFO 1192 --- [ntainer#0-0-C-1] o.a.k.c.c.internals.ConsumerCoordinator  : [Consumer clientId=consumer-2, groupId=test-consumer-group] Setting newly assigned partitions [zhisheng-0]
2019-03-29 08:40:11.544  INFO 1192 --- [ntainer#0-0-C-1] o.a.k.c.consumer.internals.Fetcher       : [Consumer clientId=consumer-2, groupId=test-consumer-group] Resetting offset for partition zhisheng-0 to offset 1.
2019-03-29 08:40:11.544  INFO 1192 --- [ntainer#0-0-C-1] o.s.k.l.KafkaMessageListenerContainer    : partitions assigned: [zhisheng-0]
2019-03-29 08:40:14.454  INFO 1192 --- [           main] com.example.demo.kafka.Producer          : +++++++++++++++++++++  message = {"id":1553820014454,"msg":"a23cdbca-5e36-474d-ba3c-5c28bdff23ce","sendTime":"Mar 29, 2019 8:40:14 AM"}
2019-03-29 08:40:14.487  INFO 1192 --- [ntainer#0-0-C-1] com.example.demo.kafka.Consumer          : ----------------- record =ConsumerRecord(topic = zhisheng, partition = 0, offset = 1, CreateTime = 1553820014454, serialized key size = -1, serialized value size = 102, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = {"id":1553820014454,"msg":"a23cdbca-5e36-474d-ba3c-5c28bdff23ce","sendTime":"Mar 29, 2019 8:40:14 AM"})
2019-03-29 08:40:14.487  INFO 1192 --- [ntainer#0-0-C-1] com.example.demo.kafka.Consumer          : ------------------ message ={"id":1553820014454,"msg":"a23cdbca-5e36-474d-ba3c-5c28bdff23ce","sendTime":"Mar 29, 2019 8:40:14 AM"}
2019-03-29 08:40:17.461  INFO 1192 --- [           main] com.example.demo.kafka.Producer          : +++++++++++++++++++++  message = {"id":1553820017461,"msg":"da152121-708f-44b8-9041-033b59382af7","sendTime":"Mar 29, 2019 8:40:17 AM"}
2019-03-29 08:40:17.463  INFO 1192 --- [ntainer#0-0-C-1] com.example.demo.kafka.Consumer          : ----------------- record =ConsumerRecord(topic = zhisheng, partition = 0, offset = 2, CreateTime = 1553820017461, serialized key size = -1, serialized value size = 102, headers = RecordHeaders(headers = [], isReadOnly = false), key = null, value = {"id":1553820017461,"msg":"da152121-708f-44b8-9041-033b59382af7","sendTime":"Mar 29, 2019 8:40:17 AM"})
2019-03-29 08:40:17.463  INFO 1192 --- [ntainer#0-0-C-1] com.example.demo.kafka.Consumer          : ------------------ message ={"id":1553820017461,"msg":"da152121-708f-44b8-9041-033b59382af7","sendTime":"Mar 29, 2019 8:40:17 AM"}
```
